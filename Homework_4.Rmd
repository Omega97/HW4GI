---
title: "HW4-Group I"
author: "Cusma Fait, Masella, El Gataa"
date: "2024-1-7"
output: html_document
---

```{r setup, include=FALSE}
library(MASS)

knitr::opts_chunk$set(echo=TRUE)
set.seed(0)
```




## FSDS - Chapter 4


### Ex 4.42

Consider $n$ independent observations from a $N(\mu, \sigma^2)$ distribution.

(a) Focusing on $\mu$, find the likelihood function. Show that the log-likelihood function is a concave, parabolic function of $\mu$. Find the ML estimator $\hat \mu$.

(b) Considering $\sigma^2$ to be known, find the information using (i) equation (4.3), (ii) equation (4.4). Use them to show that the large-sample variance of $\hat \mu$ is $\sigma^2/n$. Explain why $\hat \mu$ is the minimum variance unbiased estimator.

(c) Show that the ML estimator $\hat \sigma$ of $\sigma$ is $\sqrt{\frac{1}{n}\Sigma_i(Y_i - \bar Y)^2}$. Show that the large-sample variance of $\hat \sigma$ is $\sigma^2/2n$.

**Solution**

(a)

First, we write down the likelihood function;
$$L(\mu, \sigma^2|y) = \prod_{i=1}^n \frac{1}{\sqrt{2 \pi \sigma^2}} exp \left(-\frac{(y_i-\mu)^2}{2 \sigma^2}\right)$$
Then we use the likelihood to obtain the log-likelihood;
$$l(\mu, \sigma^2|y) = \sum_{i=1}^n log \left(\frac{1}{\sqrt{2 \pi \sigma^2}} exp \left(-\frac{(y_i-\mu)^2}{2 \sigma^2} \right) \right)$$
$$= -\frac{1}{2}\sum_{i=1}^n \left(log(2 \pi \sigma^2) +  \frac{(y_i-\mu)^2}{ \sigma^2} \right)$$

$$= -\frac{1}{2} \left(n \space log(2 \pi \sigma^2) +\frac{1}{ \sigma^2} \sum_{i=1}^n (y_i-\mu)^2 \right) $$
We can see that the log-likelihood function is a parabolic function of $\mu$.

We now proceed by calculating the derivatives;

$$\frac{\partial l(\mu, \sigma|Y)}{\partial\mu} = \frac{1}{\sigma^2} \sum_{i=1}^n (Y_i-\mu)
= n \frac{\mu - \bar Y}{\sigma^2}$$

By equating the derivative to $0$, we get the ML estimate of the mean 
$$n \frac{\hat \mu - \bar Y}{\sigma^2} = 0$$
$$\hat \mu = \frac{1}{n}\sum_{i=1}^n Y_i = \bar y$$
The ML estimator for the true mean is the sample mean.

$$\left( \frac{\partial}{\partial\mu} \right)^2 l(\mu, \sigma|Y) =  -\frac{n}{\sigma^2} < 0$$
Therefore, the log-likelihood function is also concave.


(b)

By assuming that we know $\sigma$, and by using the previous point, we can now calculate the Fisher expected information of the ML estimator $\hat \mu$ in two different ways:

$$
I(\mu|Y) 
= - \space E \left(\frac{\partial^2 l(\mu|Y)}{\partial \mu^2} \right)
= -E(-\frac{n}{\sigma^2}) 
= \frac{n}{\sigma^2}
$$
Similarly:

$$I(\mu|Y) = n \space E \left(\frac{\partial l(\mu|Y)}{\partial \mu} \right)^2$$
Since this estimator reaches the Cramér–Rao lower bound, it is considered a minimum variance estimator. 


(c)

We derive the log-likelihood from before in respect to the variance;
$$\frac{dl}{d(\sigma^2)} = -\frac{1}{2} \left( \frac{n}{\sigma^2} - \frac{1}{ \sigma^4} \Sigma_{i=1}^n (y_i-\mu)^2 \right) $$

By equating the derivative to 0 we get the ML estimator of the variance;
$$-\frac{1}{2} \left(\frac{n}{\hat \sigma^2} - \frac{1}{\hat \sigma^4} \sum_{i=1}^n (y_i-\mu)^2 \right) = 0$$
$$\hat \sigma^2 = \frac{1}{n}\sum_{i=1}^n (y_i-\bar y)^2$$
The ML estimator for the true standard deviation is the standard deviation of the sample. 

We can show that the large-sample variance of $\hat \sigma$ is $\sigma^2/2n$ with an example:

```{r 4_42_a}
sample_size <- 50
n_samples <- 1000

data <- c()
for (i in 1:n_samples){
  sample <- rnorm(sample_size)
  data[length(data)+1] <- sd(sample)
}
```

expected variance
```{r 4_42_b}
1/(2 * sample_size)
```

actual variance
```{r 4_42_c}
var(data) 
```



### Ex 4.62

For the bootstrap method, explain the similarity and difference between the true sampling distribution of $\hat \theta$ and the empirically-generated bootstrap distribution in terms of its center and its spread.

**Solution**

The following is an example that compares the distribution of $\hat \theta$ generated by sampling from the true distribution and by re-sampling the same sample multiple times with repetition (bootstrap).

```{r 4_62_a}
n <- 1000
sample_size <- 100
metric <- mean
data <- c()

for (i in 1:n) {
  y <- rnorm(sample_size)
  data[i] <- metric(y)
}

hist(data, nclass=30, main="sampling distribution of sample mean")
mean(data)
sd(data)
```

```{r 4_62_b}
y_boot <- rnorm(sample_size)
v_boot <- c()

for (i in 1:n) {
  indices <- sample.int(sample_size, sample_size, replace = TRUE)
  y <- y_boot[indices]
  v_boot[i] <- metric(y)
}

hist(v_boot, nclass=30, main="bootstrap of sample mean", col='palegreen3')
mean(v_boot)
sd(v_boot)
```

As we can see, since the sample used for the bootstrap was obtained by sampling the true distribution, both the center and the spread of the bootstrap distribution closely resemble those of the true sampling distribution of $\hat \theta$. 

This, however, is not true in general, as we can see in the following example.

```{r 4_62_c}
n <- 1000
sample_size <- 100
metric <- max
data <- c()

for (i in 1:n) {
  y <- rnorm(sample_size)
  data[i] <- metric(y)
}

hist(data, nclass=30, main="sampling distribution of sample max")
mean(data)
sd(data)
```

```{r 4_62_d}
y_boot <- rnorm(sample_size)
v_boot <- c()

for (i in 1:n) {
  indices <- sample.int(sample_size, sample_size, replace = TRUE)
  y <- y_boot[indices]
  v_boot[i] <- metric(y)
}

hist(v_boot, nclass=30, main="bootstrap of sample max", col='indianred')
mean(v_boot)
sd(v_boot)
```

In this example, the metric in question is the maximum value. Since this metric is so sensitive to the variety of values across the samples, the bootstrap method fails to capture the true spread and especially the shape of the distribution.



## FSDS - Chapter 8


### Ex 8.4

Refer to $Exercise$ $8.1$. Construct a classification tree, and prune strongly until the tree uses a single explanatory variable. Which crabs were predicted to have satellites? How does the proportion of correct predictions compare with the more complex tree in $Figure$  $8.2$?

**Solution**

...

```{r 8_4, echo=TRUE}
# Install and load the 'rpart' and 'rpart.plot' packages
install.packages(c("rpart", "rpart.plot"), repos = "http://cran.us.r-project.org")
library(rpart)
library(rpart.plot)

# Read the data
Crabs <- read.table("http://stat4ds.rwth-aachen.de/data/Crabs.dat", header=TRUE)

# Fit a classification tree
fit.tree <- rpart(y ~ weight + width + color + spine, data = Crabs)

# Prune the tree based on the cparamet
cp <- fit.tree$cptable[which.min(fit.tree$cptable[, "xerror"]), "CP"]
pruned_tree <- prune(fit.tree, cp = cp)

# Plot the pruned tree
prp(pruned_tree, extra = 1, type = 2, fallen.leaves = TRUE, main = "Pruned Classification Tree")

# Predict the response for each observation
predicted_satellites <- predict(pruned_tree, newdata = Crabs)

# Display the predictions for each crab
predicted_classes <- as.numeric(predicted_satellites)

predicted_classes
```
Comment 

The pruned classification tree, obtained by aggressively pruning until a single explanatory variable is used, provides a simplified model for predicting the presence of satellites in female horseshoe crabs. The predictions represent the classification of each observation into the binary response variable (1 for having satellites, 0 for not having). Further analysis and interpretation of the pruned tree and predictions can guide understanding of the most influential variable in the simplified model.
The variable predicted_satellites will contain the predicted response for each crab. A value of 1 indicates that the crab is predicted to have satellites, while a value of 0 indicates no satellites.
Pruning the tree strongly until it uses a single explanatory variable may result in a very simple tree, possibly just a single split based on one variable.
As we can see our tree is simplier from the one of the 8.2, our tree has just one split which is the first that predicts that all horseshoe crabs of width ≥ 26 cm don't have satellites.
...



## LAB

Suppose you receive $n=15$ phone calls in a day, and you want to build a model to assess their average length. Your likelihood for each call length is $y_i∼Exponential(\lambda)$. Now, you have to choose the prior $\pi(\lambda)$. Please, tell which of these priors is adequate to describe the problem, and provide a short motivation for each of them:

- $\pi(\lambda)=Beta(4,2)$

- $\pi(\lambda)=Normal(1,2)$

- $\pi(\lambda)=Gamma(4,2)$ 

Now, compute your posterior as $\pi(\lambda|y) \space \propto \space L(\lambda;y)\pi(\lambda)$ for the selected prior. If your first choice was correct, you will be able to compute it analytically.

**Solution**


The choice of prior distribution depends on the nature of the problem and the parameter we're estimating, in this case the rate parameter $\lambda$ of an exponential distribution.
Let's look at all the options and see which one suits our needs better:

- $\beta(4,2)$: The beta distribution is defined on the interval $[0,1]$, so it's not suitable as a prior for $\lambda$, which can take any positive value.

- $N(1,2)$: The normal distribution is defined on the entire real line, so it allows negative values of $\lambda$, which doesn't make sense in this context.

- $\gamma(4,2)$: The gamma distribution is defined on the positive real line, making it a suitable choice for a rate parameter $\lambda$. It's often used as a prior for the rate parameter of an exponential distribution because it's conjugate to the exponential likelihood, meaning that the posterior distribution is also a Gamma distribution.

In fact, given the exponential likelihood and a gamma prior, the posterior distribution can be calculated analytically. If $y_1,...,y_n$ are the call lengths and $λ \sim \gamma(a,b)$, then the posterior distribution of $\lambda$ is given by

$$
\begin{split}
  \pi(\lambda|y) \space &\propto \space L(\lambda;y)\pi(\lambda;\alpha,\beta)\\
  &\propto (\lambda^ne^{-\lambda n \bar y}) \times (\lambda^{\alpha-1}e^{-\beta\lambda})\\
  &=\lambda^{n+\alpha-1}e^{-\lambda(n\bar y+\beta)}
\end{split}
$$
As we can see, the posterior distribution is recognised as a gamma distribution with parameters $\gamma(\alpha+n,\beta+n\bar y)$, where $\bar y$ is the mean of the call length $y_i$.



## ISLR - chapter 7


### exercises 7.9



**Solution**


```{r 7_9, echo=TRUE}
# Load the necessary library and data
library(MASS)
data(Boston)

# Subset the data
bos_subset <- Boston[, c("dis", "nox")]

# Fit a cubic polynomial regression
poly_fit <- lm(nox ~ poly(dis, 3), data = bos_subset)

# Display regression output
summary(poly_fit)

# Plot the data and polynomial fit
plot(bos_subset$dis, bos_subset$nox, col = "blue", pch = 20, main = "Cubic Polynomial Regression", xlab = "dis", ylab = "nox")
lines(sort(bos_subset$dis), fitted(poly_fit)[order(bos_subset$dis)], col = "red", lwd = 2)
# Function to fit polynomial and return residual sum of squares
fit_poly_and_return_rss <- function(degree) {
  poly_fit <- lm(nox ~ poly(dis, degree), data = bos_subset)
  rss <- sum(residuals(poly_fit)^2)
  return(rss)
}

# Plot polynomial fits for degrees 1 to 10
degrees <- 1:10
rss_values <- sapply(degrees, fit_poly_and_return_rss)

plot(degrees, rss_values, type = "b", main = "Residual Sum of Squares for Polynomial Fits",
     xlab = "Polynomial Degree", ylab = "Residual Sum of Squares")

# Load necessary libraries
library(MASS)
library(boot)

# Subset the data
bos_subset <- Boston[, c("dis", "nox")]

# Function to fit polynomial and return cross-validation error
fit_poly_and_return_cv_error <- function(degree) {
  poly_fit <- lm(nox ~ poly(dis, degree), data = bos_subset)
  cv_fit <- cv.glm(bos_subset, poly_fit)
  return(cv_fit$delta[1])
}

# Perform cross-validation to select optimal degree
degrees <- 1:10
cv_errors <- sapply(degrees, fit_poly_and_return_cv_error)

# Find the degree with the minimum cross-validation error
optimal_degree <- which.min(cv_errors)

# Display optimal degree
cat("Optimal Degree:", optimal_degree, "\n")

# Fit regression spline with 4 degrees of freedom
library(splines)
spline_fit <- lm(nox ~ bs(dis, df = 4), data = bos_subset)

# Display output
summary(spline_fit)

# Plot the data and spline fit
plot(bos_subset$dis, bos_subset$nox, col = "blue", pch = 20, main = "Regression Spline (4 df)", xlab = "dis", ylab = "nox")
lines(sort(bos_subset$dis), fitted(spline_fit)[order(bos_subset$dis)], col = "red", lwd = 2)
# Function to fit spline and return residual sum of squares
fit_spline_and_return_rss <- function(df) {
  spline_fit <- lm(nox ~ bs(dis, df = df), data = bos_subset)
  rss <- sum(residuals(spline_fit)^2)
  return(rss)
}

# Plot spline fits for degrees of freedom from 1 to 10
df_values <- 1:10
rss_spline_values <- sapply(df_values, fit_spline_and_return_rss)

plot(df_values, rss_spline_values, type = "b", main = "Residual Sum of Squares for Spline Fits",
     xlab = "Degrees of Freedom", ylab = "Residual Sum of Squares")

# Perform cross-validation to select the best degrees of freedom
#cv_errors_spline <- sapply(df_values, function(df) {
#  spline_fit <- lm(get("nox") ~ bs(dis, df = df), data = bos_subset)
#  cv_fit_spline <- cv.glm(bos_subset, spline_fit)  # Use cv.glm() instead of cv.lm()
#  return(cv_fit_spline$delta[1])
#})

# Find the degrees of freedom with the minimum cross-validation error
#optimal_df <- which.min(cv_errors_spline)

# Display optimal degrees of freedom
#cat("Optimal Degrees of Freedom:", optimal_df, "\n")
```

a) Fit a Cubic Polynomial Regression:
Results: Understand the relationship between distances to employment centers (dis) and nitrogen oxide concentration (nox). The fitted cubic polynomial model showcases any non-linear patterns.

(b) Plot Polynomial Fits for Different Degrees:
Results: Explore how the model fit changes with different polynomial degrees. The curve illustrates the trade-off between model complexity and goodness of fit.

(c) Perform Cross-Validation for Polynomial Degree Selection:
Results: Identify the optimal polynomial degree that balances model complexity and generalization performance using cross-validation.

(d) Use the bs() Function to Fit a Regression Spline:
Results: Capture non-linear relationships with a spline. The spline curve visually represents how the model adapts to the complexities in the data.

(e) Fit a Regression Spline for a Range of Degrees of Freedom:
Results: Observe how spline flexibility affects model fit. The curve shows the trade-off between fitting closely to the data and avoiding overfitting.

(f) Perform Cross-Validation for Spline Degree Selection:
Results: Choose the optimal degrees of freedom for the spline model using cross-validation. Ensures a balance between flexibility and generalization.
...



## GAM

This question is about using $gam$ for univariate smoothing, the advantages of penalized regression and weighting a smooth model fit. The $mcycle$ data in the $MASS$ package are a classic dataset in univariate smoothing, introduced in Silverman (1985). The data measure the acceleration of the rider’s head, against time, in a simulated motorcycle crash.

1. Plot the acceleration against time, and use $gam$ to fit a univariate smooth to the data, selecting the smoothing parameter by GCV (k of 30 to 40 is plenty for this example). Plot the resulting smooth, with partial residuals, but without standard errors.

2. Use $lm$ and $poly$ to fit a polynomial to the data, with approximately the same degrees of freedom as was estimated by $gam$. Use $termplot$ to plot the estimated polynomial and partial residuals. Note the substantially worse fit achieved by the polynomial, relative to the penalized regression spline fit.

3. It’s possible to overstate the importance of penalization in explaining the improvement of the penalized regression spline, relative to the polynomial. Use $gam$ to refit an un-penalized thin plate regression spline to the data, with basis dimension the same as that used for the polynomial, and again produce a plot for comparison with the previous two results.

4. Redo part 3 using an un-penalized cubic regression spline. You should find a fairly clear ordering of the acceptability of the results for the four models tried - what is it?

5. Now plot the model residuals against time, and comment.

6. Fit a linear model including a b-spline using the function bs on times and select a suitable degree and the knots position. Compare this model with the previous ones and comment.

**Solution**

1. First of all, let's take a look to our dataset

```{r GAM.0, include=FALSE}
# Load the packages
library(MASS)
library(mgcv)
library(splines)
```

```{r GAM.1.1, echo=TRUE}

# Load the mcycle data
data(mcycle)

# Plot the data
plot(mcycle$times, mcycle$accel, xlab = "Time (ms)", ylab = "Acceleration (g)",
     main = "Acceleration vs Time in a Simulated Motorcycle Accident")
```
And now we can create the require GAM model and see how it fit our data:

```{r GAM.1.2, echo=TRUE}
# Fit the model
fit_gam <- gam(accel ~ s(times, k = 35), data = mcycle, method = "GCV.Cp")

# Print the summary of the model
summary(fit_gam)

# Plot the smooth
plot(fit_gam, residuals = TRUE, se = FALSE)
```

2. Taking a look to the summary from the previous point we can see that edf (estimated degrees of freedom) of the model was around 11, so we'll use it here

```{r GAM.2.1, echo=TRUE}
# Fit a polynomial to the data
fit_poly <- lm(accel ~ poly(times, degree =11, raw = TRUE), data = mcycle)

# Print the summary of the model
summary(fit_poly)

# Plot the estimated polynomial and partial residuals
termplot(fit_poly, partial.resid = TRUE, se = TRUE)
```

3. Here's how I did it:

```{r GAM.3.1, echo=TRUE}
# Fit the model
fit_unpenalized <- gam(accel ~ s(times, bs = "tp"), data = mcycle)

# Print the summary of the model
summary(fit_unpenalized)

# Plot the estimated spline
plot(fit_unpenalized, residuals = TRUE, se = FALSE)
```

4.

```{r GAM.4.1, echo=TRUE}
# Fit the model
fit_cubic <- gam(accel ~ s(times, bs = "cr"), data = mcycle)

# Print the summary of the model
summary(fit_cubic)

# Plot the estimated spline
plot(fit_cubic, residuals = TRUE, se = FALSE)
```
Looking at all four fits, you can see that the fit function is becoming smoother and better suited to our data set.

5. Looking at a quick comparison between these four, we can see that the first model has a better distribution around 0 in the first part, better than the last two, but in general they don't seem to have any particular trend.

```{r GAM.5.1, echo=TRUE}
par(mfrow=c(2,2))  # 2 rows, 2 columns

# For the gam model
plot(mcycle$times, residuals(fit_gam), xlab = "Time (ms)", ylab = "Residuals",
     main = "Residuals vs Time for the GAM Model")

# For the polynomial model
plot(mcycle$times, residuals(fit_poly), xlab = "Time (ms)", ylab = "Residuals",
     main = "Residuals vs Time for the Polynomial Model")

# For the unpenalized thin plate regression spline model
plot(mcycle$times, residuals(fit_unpenalized), xlab = "Time (ms)", ylab = "Residuals",
     main = "Residuals vs Time for the Unpenalized Thin Plate Regression Spline Model")

# For the unpenalized cubic regression spline model
plot(mcycle$times, residuals(fit_cubic), xlab = "Time (ms)", ylab = "Residuals",
     main = "Residuals vs Time for the Unpenalized Cubic Regression Spline Model")
```


6. Honestly, I wanted to try to fit it using a B-splines as basis functions with only one degree in 4 different knots, chosen by me, looking at the plot at the beginning and taking into consideration how the results changes in better.

```{r GAM.6.1, echo=TRUE}
par(mfrow=c(1,1)) 

# Fit a linear model with a B-spline basis
fit_bs <- lm(accel ~ bs(times, degree = 1, knots = c(13.5,21,32,40)), data = mcycle)

# Print the summary of the model
summary(fit_bs)

# Plot the estimated polynomial and partial residuals
termplot(fit_bs, partial.resid = TRUE, se = TRUE)

# Plot the residuals
plot(mcycle$times, residuals(fit_bs), xlab = "Time (ms)", ylab = "Residuals",
     main = "Residuals vs Time for the B-Spline Model")
```
I wanted to compare all the five models calculating the Akaike Information Criterion (AIC) and, as you can see:
```{r GAM.6.2, echo=TRUE}
print(AIC(fit_gam))
print(AIC(fit_poly))
print(AIC(fit_unpenalized))
print(AIC(fit_cubic))
print(AIC(fit_bs))
```
Looking at the value, except for the one where I used the polynomial, the model I created has a value very close to the minimum two given by the one with the unpenalised thin-plate regression spline model and the unpenalised cubic regression spline model.
So this model, especially because it is the one with fewer degrees of freedom, can also be considered a good one.

